{"meta":{"title":"Yu Hanqing's Blog","subtitle":null,"description":null,"author":"Yu Hanqing","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2019-10-28T08:11:52.236Z","updated":"2019-10-28T08:11:52.236Z","comments":true,"path":"404.html","permalink":"http://yoursite.com/404.html","excerpt":"","text":"404 Page 404 找不到页面 返回上一页 返回首页"},{"title":"分类","date":"2019-10-28T07:29:43.000Z","updated":"2019-10-28T08:16:26.773Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-10-24T15:22:28.000Z","updated":"2019-10-24T15:22:57.795Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2019-10-28T08:09:04.522Z","updated":"2019-02-14T22:23:50.000Z","comments":true,"path":"404_css/style.css","permalink":"http://yoursite.com/404_css/style.css","excerpt":"","text":"body { background-color: #2F3242; } svg { position: absolute; top: 50%; left: 50%; margin-top: -250px; margin-left: -400px; } .message-box { height: 200px; width: 380px; position: absolute; top: 50%; left: 50%; margin-top: -100px; margin-left: 50px; color: #FFF; font-family: Roboto; font-weight: 300; } .message-box h1 { font-size: 60px; line-height: 46px; margin-bottom: 40px; } .buttons-con .action-link-wrap { margin-top: 40px; } .buttons-con .action-link-wrap a { background: #68c950; padding: 8px 25px; border-radius: 4px; color: #FFF; font-weight: bold; font-size: 14px; transition: all 0.3s linear; cursor: pointer; text-decoration: none; margin-right: 10px } .buttons-con .action-link-wrap a:hover { background: #5A5C6C; color: #fff; } #Polygon-1 , #Polygon-2 , #Polygon-3 , #Polygon-4 , #Polygon-4, #Polygon-5 { -webkit-animation: float 1s infinite ease-in-out alternate; animation: float 1s infinite ease-in-out alternate; } #Polygon-2 { -webkit-animation-delay: .2s; animation-delay: .2s; } #Polygon-3 { -webkit-animation-delay: .4s; animation-delay: .4s; } #Polygon-4 { -webkit-animation-delay: .6s; animation-delay: .6s; } #Polygon-5 { -webkit-animation-delay: .8s; animation-delay: .8s; } @-webkit-keyframes float { 100% { -webkit-transform: translateY(20px); transform: translateY(20px); } } @keyframes float { 100% { -webkit-transform: translateY(20px); transform: translateY(20px); } } @media (max-width: 450px) { svg { position: absolute; top: 50%; left: 50%; margin-top: -250px; margin-left: -190px; } .message-box { top: 50%; left: 50%; margin-top: -100px; margin-left: -190px; text-align: center; } }"}],"posts":[{"title":"蒙特卡洛树搜索MCTS入门","slug":"蒙特卡洛树搜索MCTS入门","date":"2019-10-28T07:46:02.000Z","updated":"2019-10-28T13:38:24.674Z","comments":true,"path":"2019/10/28/蒙特卡洛树搜索MCTS入门/","link":"","permalink":"http://yoursite.com/2019/10/28/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2MCTS%E5%85%A5%E9%97%A8/","excerpt":"蒙特卡洛树搜索的简单介绍","text":"蒙特卡洛树搜索的简单介绍 蒙特卡洛树搜索MCTS入门【内容简介】蒙特卡洛树搜索( Monte Carlo Tree Search )是一种寻找最优决策的方法，在AlphaGo中被运用，其主要分为四步：选择(Selection)，拓展(Expansion)，模拟(Simulation)，反向传播(Backpropagation)。 下面我们以井字棋为例对这一方法进行介绍。 基础知识节点在棋类问题中，MCTS使用一个节点来表示一个游戏状态。若现在井字棋的棋盘上只有中间一个棋子，我们用一个节点表示这个游戏状态。那么，下一步棋有8种下法，所以对应的，刚才那个节点就有8个子节点(图中只画出了3个)，如下图所示。根据这个方法，这些子节点又有子节点，所有的井字棋游戏状态都可以被这样表示，于是它们就构成了一个树。蒙特卡洛树搜索就是要在这样一个树中搜索最可能获胜的游戏状态，即搜索最佳下棋策略。 节点的两个属性在蒙特卡洛树搜索中，节点记作$v$，且每个节点都有如下属性。 $N(v)$：节点$v$的访问次数，节点被访问多少次，该值就是多少。 $Q(v)$：节点$v$的累计奖励，即节点在反向传播过程中所有奖励的和。 这两个属性会在下面的过程中用到，并会详细说明。 搜索过程那么，给定当前游戏状态，如何获得下一步的最佳下法呢？对于井字棋来说，当然可以遍历所有可能性，直接找出最优策略。但若换成围棋等复杂的棋类，遍历的方法显然不可行。下面就是蒙特卡洛树搜索的核心部分。 选择(Selection)选择阶段，从根节点(给定的游戏状态)出发，根据一定的策略，向下选择一个节点，若被选择的节点未被访问，则执行扩展；若被选择的节点已被访问，则继续向下选择节点，直到遇见未被访问的节点并执行扩展，或遇见终止节点(游戏结束)，无需执行扩展，而直接执行反向传播。 这个选择的策略由该公式确定，对于当前节点的每个子节点计算如下公式，并选择结果最大的节点。$$\\underset{v’\\in ,,\\text{children of }v}{arg\\max}\\frac{Q\\left( v’ \\right)}{N\\left( v’ \\right)}+c\\sqrt{\\frac{\\text{2}\\ln N\\left( v \\right)}{N\\left( v’ \\right)}}$$其中， $v$ 表示父节点，$v’$ 表示子节点。$c$是一常数，用于权衡探索(Exploration)与利用(Exploitation)。$c$越大，算法就越偏向于探索访问次数少的节点；$c$越小，算法就越偏向于选择获胜可能性高的节点，称为利用。 扩展(Expansion)扩展非常简单，就是把选择步骤中遇到的未访问节点添加到现有的树中。 模拟(Simulation)从被扩展的节点开始，让游戏随机进行，也就是在棋盘上随机下棋，直到游戏结束。若此时游戏胜利，则奖励(Reward)为$1$；若游戏失败，奖励为$0$。这个奖励会在反向传播中被用到。 注：在其他应用中，奖励也可是其他值。 反向传播(Backpropagation)反向传播其实就是对执行模拟的节点或终止节点的所有父节点的两个属性进行逐一更新的过程。 将奖励记作$R$，对所有节点$v$，包括当前节点，都执行以下操作。$$N(v)=N(v)+1 \\\\Q(v)=Q(v)+R$$我们再回头看看选择步骤中的公式$$\\underset{v’\\in ,,\\text{children of }v}{arg\\max}\\frac{Q\\left( v’ \\right)}{N\\left( v’ \\right)}+c\\sqrt{\\frac{\\text{2}\\ln N\\left( v \\right)}{N\\left( v’ \\right)}}$$可以看到，式中第一项其实就是该节点在前面的过程中获得的平均奖励，自然该值越大，选择该节点就越有可能获胜。那么为什么要加上第二项呢？这就涉及到探索与利用的权衡。 我们不能只选择已访问过的节点中平均奖励最大的节点，一些访问次数较少的、甚至没有访问过的节点它们可能获得比已探索的节点更丰厚的回报，因此也要适当地在未知的节点进行探索。这就是第二项的含义，当该节点访问次数占父节点次数的比例越小时，该值越大，代表我们越要在此节点进行探索。于是就不难理解$c$用于是权衡探索与利用的常数了。 这就是上限置信区间算法(Upper Confidence Bound )，简称UCT算法。 搜索过程展示看完了这四个步骤，我们再来看一张动图帮助理解。图中节点内数字表示$Q(v)/N(v)$ 搜索结束MCTS的整个搜索过程就是这样，那么什么时候结束呢？一般设置以下两个终止条件。 设置最大根节点搜索次数，达到该次数后结束搜索。 设置最大搜索时间，超过时间后结束搜索。 选择最佳节点搜索结束后，选择哪个节点作为下一步的选择呢？ 不是选择$Q$最大的节点，也不是选择平均奖励最大的节点，而是选择访问次数最多的节点。这样，就得到了当前游戏状态(根节点)下的一个选择。 如果下一步还要进行决策，则又要将下一步的状态作为根节点，重新执行MCTS搜索，并选择访问次数最多的节点作为下一步的策略。(上一步的搜索结果可以保留) 以上只是MCTS的简单介绍，想更详细的了解MCTS可以参考论文A Survey of Monte Carlo Tree Search Methods 另外，Github上也已经有MCTS的Python实现源码 https://github.com/pbsinclair42/MCTS","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]}]}