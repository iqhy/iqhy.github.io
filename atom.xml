<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yu Hanqing&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-02-28T05:04:56.625Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Yu Hanqing</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>蒙特卡洛树搜索MCTS入门</title>
    <link href="http://yoursite.com/2019/10/28/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2MCTS%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2019/10/28/蒙特卡洛树搜索MCTS入门/</id>
    <published>2019-10-28T07:46:02.000Z</published>
    <updated>2020-02-28T05:04:56.625Z</updated>
    
    <content type="html"><![CDATA[<p>【内容简介】<strong>蒙特卡洛树搜索</strong>(Monte Carlo Tree Search) 是一种寻找最优决策的方法，在AlphaGo中被运用，其主要分为四步：<strong>选择</strong>(Selection)，<strong>拓展</strong>(Expansion)，<strong>模拟</strong>(Simulation)，<strong>反向传播</strong>(Backpropagation)。 本文以<strong>井字棋</strong>为例对这一方法进行介绍。</p><a id="more"></a><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="节点"><a href="#节点" class="headerlink" title="节点"></a>节点</h3><p>在棋类问题中，MCTS 使用一个<strong>节点</strong>来表示一个<strong>游戏状态</strong>。若现在井字棋的棋盘上只有中间一个棋子，我们用一个节点表示这个游戏状态。那么，下一步棋有<strong>8种下法</strong>，所以对应的，刚才那个节点就有<strong>8个子节点</strong>(图中只画出了3个)，如下图所示。根据这个方法，这些子节点又有子节点，所有的井字棋游戏状态都可以被这样表示，于是它们就构成了一个树。蒙特卡洛树搜索就是要在这样一个树中搜索最可能获胜的游戏状态，即搜索最佳<strong>下棋策略</strong>。</p>  <div align="center">  <img src="/2019/10/28/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2MCTS%E5%85%A5%E9%97%A8/节点.png" alt="节点" width="250"></div><h3 id="节点的两个属性"><a href="#节点的两个属性" class="headerlink" title="节点的两个属性"></a>节点的两个属性</h3><p>在蒙特卡洛树搜索中，节点记作$v$，且每个节点都有如下属性。</p><ol><li>$N(v)$：节点$v$的<strong>访问次数</strong>，节点被访问多少次，该值就是多少。</li><li>$Q(v)$：节点$v$的<strong>累计奖励</strong>，即节点在<strong>反向传播</strong>过程中所有奖励的和。</li></ol><p>这两个属性会在下面的过程中用到，并会详细说明。</p><h2 id="搜索过程"><a href="#搜索过程" class="headerlink" title="搜索过程"></a>搜索过程</h2><p>那么，给定当前游戏状态，如何获得下一步的最佳下法呢？对于井字棋来说，当然可以遍历所有可能性，直接找出最优策略。但若换成围棋等复杂的棋类，遍历的方法显然不可行。下面就是蒙特卡洛树搜索的核心部分。</p><h3 id="选择-Selection"><a href="#选择-Selection" class="headerlink" title="选择(Selection)"></a>选择(Selection)</h3><p>选择阶段，从根节点(给定的游戏状态)出发，根据一定的<strong>策略</strong>，向下选择一个节点，若被选择的节点<strong>未被访问</strong>，则执行<strong>扩展</strong>；若被选择的节点<strong>已被访问</strong>，则继续向下选择节点，直到遇见未被访问的节点并执行<strong>扩展</strong>，或遇见终止节点(游戏结束)，无需执行<strong>扩展</strong>，而直接执行<strong>反向传播</strong>。</p><p>这个选择的策略由该公式确定，对于当前节点的每个子节点计算如下公式，并选择结果最大的节点。<br>$$<br>\underset{v’\in ,,\text{children of }v}{arg\max}\frac{Q\left( v’ \right)}{N\left( v’ \right)}+c\sqrt{\frac{\text{2}\ln N\left( v \right)}{N\left( v’ \right)}}<br>$$<br>其中， $v$ 表示父节点，$v’$ 表示子节点。$c$是一常数，用于权衡<strong>探索 (Exploration)</strong> 与<strong>利用 (Exploitation)</strong>。$c$越大，算法就越偏向于<strong>探索</strong>访问次数少的节点；$c$越小，算法就越偏向于选择获胜可能性高的节点，称为<strong>利用</strong>。</p><h3 id="扩展-Expansion"><a href="#扩展-Expansion" class="headerlink" title="扩展 (Expansion)"></a>扩展 (Expansion)</h3><p>扩展非常简单，就是把<strong>选择</strong>步骤中遇到的未访问节点添加到现有的树中。</p><h3 id="模拟-Simulation"><a href="#模拟-Simulation" class="headerlink" title="模拟 (Simulation)"></a>模拟 (Simulation)</h3><p>从被扩展的节点开始，让游戏<strong>随机</strong>进行，也就是在棋盘上随机下棋，直到<strong>游戏结束</strong>。若此时游戏胜利，则<strong>奖励 (Reward)</strong> 为$1$；若游戏失败，<strong>奖励</strong>为$-1$。这个奖励会在<strong>反向传播</strong>中被用到。</p><p>注：在其他应用中，<strong>奖励</strong>也可是其他值。</p><h3 id="反向传播-Backpropagation"><a href="#反向传播-Backpropagation" class="headerlink" title="反向传播 (Backpropagation)"></a>反向传播 (Backpropagation)</h3><p>反向传播其实就是对<strong>执行模拟的节点</strong>或<strong>终止节点</strong>的所有父节点的<strong>两个属性</strong>进行逐一更新的过程。</p><p>将奖励记作$R$，对所有节点$v$，包括当前节点，都执行以下操作。<br>$$<br>N(v)=N(v)+1  \\<br>Q(v)=Q(v)+R<br>$$<br>我们再回头看看<strong>选择</strong>步骤中的公式<br>$$<br>\underset{v’\in ,,\text{children of }v}{arg\max}\frac{Q\left( v’ \right)}{N\left( v’ \right)}+c\sqrt{\frac{\text{2}\ln N\left( v \right)}{N\left( v’ \right)}}<br>$$<br>可以看到，式中<strong>第一项</strong>其实就是该节点在前面的过程中获得的<strong>平均奖励</strong>，自然该值越大，选择该节点就越有可能获胜。那么为什么要加上第二项呢？这就涉及到<strong>探索</strong>与<strong>利用</strong>的权衡。</p><p>我们不能只选择已访问过的节点中<strong>平均奖励</strong>最大的节点，一些<strong>访问次数较少</strong>的、甚至<strong>没有访问过</strong>的节点它们可能获得比已探索的节点更丰厚的回报，因此也要适当地在未知的节点进行探索。这就是第二项的含义，当该节点访问次数占父节点次数的比例越小时，该值越大，代表我们越要在此节点进行探索。于是就不难理解$c$用于是权衡<strong>探索</strong>与<strong>利用</strong>的常数了。</p><p>这就是<strong>上限置信区间算法 (Upper Confidence Bound )</strong>，简称UCT算法。</p><h3 id="搜索过程展示"><a href="#搜索过程展示" class="headerlink" title="搜索过程展示"></a>搜索过程展示</h3><p>看完了这四个步骤，我们再来看一张动图帮助理解。图中节点内数字表示 $Q(v)/N(v)$</p> <div align="center">  <img src="/2019/10/28/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2MCTS%E5%85%A5%E9%97%A8/搜索过程.gif" alt="搜索过程" width="400"></div><h2 id="搜索结束"><a href="#搜索结束" class="headerlink" title="搜索结束"></a>搜索结束</h2><p>MCTS的整个搜索过程就是这样，那么什么时候结束呢？一般设置以下两个终止条件。</p><ol><li>设置最大根节点搜索次数，达到该次数后结束搜索。</li><li>设置最大搜索时间，超过时间后结束搜索。</li></ol><h2 id="选择最佳节点"><a href="#选择最佳节点" class="headerlink" title="选择最佳节点"></a>选择最佳节点</h2><p>搜索结束后，选择哪个节点作为下一步的选择呢？</p><p>不是选择$Q$最大的节点，也不是选择平均奖励最大的节点，而是选择<strong>访问次数</strong>最多的节点。这样，就得到了<strong>当前游戏状态(根节点)</strong>下的一个选择。</p><p>如果下一步还要进行决策，则又要将下一步的状态作为<strong>根节点</strong>，重新执行MCTS搜索，并选择访问次数最多的节点作为下一步的策略。(上一步的搜索结果可以保留)</p><p>以上只是 MCTS 的简单介绍，想更详细的了解 MCTS 可以参考论文<a href="http://www.researchgate.net/publication/235985858_A_Survey_of_Monte_Carlo_Tree_Search_Methods" target="_blank" rel="noopener">A Survey of Monte Carlo Tree Search Methods</a></p><p>另外，Github 上也已经有 MCTS 的 Python 实现源码 <a href="https://github.com/pbsinclair42/MCTS" target="_blank" rel="noopener">https://github.com/pbsinclair42/MCTS</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;【内容简介】&lt;strong&gt;蒙特卡洛树搜索&lt;/strong&gt;(Monte Carlo Tree Search) 是一种寻找最优决策的方法，在AlphaGo中被运用，其主要分为四步：&lt;strong&gt;选择&lt;/strong&gt;(Selection)，&lt;strong&gt;拓展&lt;/strong&gt;(Expansion)，&lt;strong&gt;模拟&lt;/strong&gt;(Simulation)，&lt;strong&gt;反向传播&lt;/strong&gt;(Backpropagation)。 本文以&lt;strong&gt;井字棋&lt;/strong&gt;为例对这一方法进行介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
</feed>
